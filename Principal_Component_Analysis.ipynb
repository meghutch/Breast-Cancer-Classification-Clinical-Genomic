{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Principal Component Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meghutch/Breast-Cancer-Classification-Clinical-Genomic/blob/master/Principal_Component_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBqDJMLTSfPr",
        "colab_type": "text"
      },
      "source": [
        "# **Predicting Clinical Outcomes of Breast Cancer Patients**\n",
        "\n",
        "## **Neural Networks and PCA**\n",
        "\n",
        "**Author:** Meg Hutch\n",
        "\n",
        "**Date:** November 27, 2019\n",
        "\n",
        "**Objective:** Integrate the Principal Components from the **Gene Expression Analysis - PCA**. \n",
        "\n",
        "These were the Principal Components explaining 90% of the variance. \n",
        "\n",
        "Unlike the **Neural_Network_Clinical_Outcomes** analysis, this will use the patients in the merged_expression.txt that were processed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pG01kP6UVC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import seaborn as sns\n",
        "\n",
        "# import the metrics class sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel # to identify features\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mamswLh8QkLR",
        "colab_type": "code",
        "outputId": "fce7c4ae-43dd-400f-f7da-6aa4001276b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Connect Colab to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ltCT9Zv8LAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "# Merged Expression Data\n",
        "gene_data = pd.read_csv('/content/drive/My Drive/Projects/Breast_Cancer_Classification/Data/merged_expression.txt', sep=',')\n",
        "\n",
        "## Principal Component Data\n",
        "# All Principal Components\n",
        "pc_all = pd.read_csv('/content/drive/My Drive/Projects/Breast_Cancer_Classification/Processed_Data/gene_pca_components_All_1747.txt')\n",
        "\n",
        "# Principal Components Responsible for 90% of the variation\n",
        "pc_90 = pd.read_csv('/content/drive/My Drive/Projects/Breast_Cancer_Classification/Processed_Data/gene_pca_components_90.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNv2A3jUNQr",
        "colab_type": "text"
      },
      "source": [
        "# **Data Pre-Processing**\n",
        "\n",
        "Check if there are missing values and add the event label to the prinicipal component dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_29Kdk_59QqC",
        "colab_type": "text"
      },
      "source": [
        "**Check if there are any missing values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2sjClIA9QOq",
        "colab_type": "code",
        "outputId": "05963be8-e2e1-4e89-c16b-b0bc952c0076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "gene_data.isna().any()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0    False\n",
              "EVENT          True\n",
              "OS_MONTHS     False\n",
              "FIVE_YEAR      True\n",
              "RERE          False\n",
              "              ...  \n",
              "CC2D1A        False\n",
              "CB986545      False\n",
              "IGSF9         False\n",
              "DA110839      False\n",
              "FAM71A         True\n",
              "Length: 24372, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjHaZnph9V3A",
        "colab_type": "text"
      },
      "source": [
        "**Remove observations with missing values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f85OtiV9yy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gene_data = gene_data.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp6ELmfIHKG6",
        "colab_type": "text"
      },
      "source": [
        "**Check Final Number of Patients**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuGUGuNmKB0r",
        "colab_type": "code",
        "outputId": "1a215688-add3-4018-c366-89ca9726c126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "gene_data.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1747 entries, 0 to 1903\n",
            "Columns: 24372 entries, Unnamed: 0 to FAM71A\n",
            "dtypes: float64(24371), object(1)\n",
            "memory usage: 324.9+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54mQorOOeCeR",
        "colab_type": "text"
      },
      "source": [
        "**Add Outcome Variable to the PC Dataframes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZq4Ba5NeB5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subset only the Event - whether they lived (0) or died (1) from Breast Cancer\n",
        "labels = gene_data.EVENT\n",
        "\n",
        "# Create a list of row names\n",
        "patients = list(gene_data.index)\n",
        "\n",
        "# Convert labels into a dataframe and indicate patients as the index\n",
        "labels = pd.DataFrame(labels, index = patients)\n",
        "\n",
        "# Create a row ID from the row names \n",
        "#labels['ID'] = np.arange(len(labels))\n",
        "\n",
        "pc_all = pc_all.set_index('Unnamed: 0')\n",
        "pc_90 = pc_90.set_index('Unnamed: 0')\n",
        "\n",
        "# Remove the index name (Unnamed: 0)\n",
        "pc_all.index.name = None\n",
        "pc_90.index.name = None\n",
        "\n",
        "# Add labels to the pc dataframes\n",
        "pc_all = pd.merge(pc_all, labels, left_index=True, right_index=True)\n",
        "pc_90 = pd.merge(pc_90, labels, left_index=True, right_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1t8xyhNmSX2",
        "colab_type": "text"
      },
      "source": [
        "# **Predict Outcomes Using Gene Expression Principal Components**\n",
        "\n",
        "We will attempt to predict Event (1 = died from breast cancer, 0 = alive), using the following classification methods and only using Principal Components as features\n",
        "\n",
        "* Logistic Regression\n",
        "* Random Forest\n",
        "* Neural Networks. \n",
        "\n",
        "Logistic regression and random forest classiers can help serve as a benchmark of performance once we develop our neural network classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKzlNG-xu4EN",
        "colab_type": "text"
      },
      "source": [
        "# **Training and Testing Split**\n",
        "\n",
        "For all of our classification methods, we will create a training and a testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxdYyZxEujxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Packagess\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKwf3jzovYN0",
        "colab_type": "text"
      },
      "source": [
        "Format the pc data into a features and label dataframe. EVENT indicates that the patient died from disease. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H421ISlCvf4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let x represent the input features; y the labels\n",
        "\n",
        "# Want to remove EVENT from inputs since these are what we are trying to predict. Event will remain as our label.\n",
        "\n",
        "# create x to represent the input features; y is the label\n",
        "x = pc_90.drop(['EVENT'], axis=1)\n",
        "y = pc_90.EVENT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jot8CaTpvfEP",
        "colab_type": "text"
      },
      "source": [
        "Split the data into testing and training sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuPWs5pQv5d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7UGiy4twHbY",
        "colab_type": "text"
      },
      "source": [
        "View the shapes of the training and testing sets; the datasets ending in \"train\" are our training sets; similarly, those ending in \"test\" are the testing; x prefix always represents the input features, y the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujubXxyzwU14",
        "colab_type": "code",
        "outputId": "1647aaa2-b44a-447d-8980-1b7b9faa4d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "# Assess the trainig and testing sets previously created\n",
        "print('Training Features Shape:', X_train.shape)\n",
        "print('Training Labels Shape:', y_train.shape)\n",
        "print('Testing Features Shape:', X_test.shape)\n",
        "print('Testing Labels Shape:', y_test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Features Shape: (1310, 1213)\n",
            "Training Labels Shape: (1310,)\n",
            "Testing Features Shape: (437, 1213)\n",
            "Testing Labels Shape: (437,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u1NBiEwYeS",
        "colab_type": "text"
      },
      "source": [
        "Examine the class distributions - these are similar splits between the training and testing sets. This is also representative of the number of cases throughout the entire dataset. \n",
        "\n",
        "Overall, 35% of the patients in our total data died from disease. So this is an unbalanced dataset and a few percents off from the splits in the training/testing set. **Not sure if this is a problem?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzAqXr4pwX3M",
        "colab_type": "code",
        "outputId": "84cb5b69-2d7b-437c-d628-1a62e0a514e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "# Died from disease\n",
        "print('Died of Diseases in Training Set:', np.count_nonzero(y_train == 1))\n",
        "print('Died of Diseases in Testing Set:', np.count_nonzero(y_test == 1))\n",
        "\n",
        "# Percents\n",
        "print('% Died of Diseases Cases in Training Set:', round(np.count_nonzero(y_train == 1)/1139*100,2))\n",
        "print('% Died of Diseases Cases in Testing Set:', round(np.count_nonzero(y_test == 1)/380*100,2))\n",
        "\n",
        "# Died of disease overall\n",
        "print('Died of Diseases Overall:', np.count_nonzero(pc_90 == 1))\n",
        "print('% Died of Diseases Overall:', round(np.count_nonzero(pc_90 == 1)/1747*100,2))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Died of Diseases in Training Set: 470\n",
            "Died of Diseases in Testing Set: 150\n",
            "% Died of Diseases Cases in Training Set: 41.26\n",
            "% Died of Diseases Cases in Testing Set: 39.47\n",
            "Died of Diseases Overall: 620\n",
            "% Died of Diseases Overall: 35.49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUc2GBdH5wiL",
        "colab_type": "text"
      },
      "source": [
        "# **Logistic Regression**\n",
        "\n",
        "Because we don't want to overfit the logistic regression model or have problems with convergence, we will follow the 1 in 10 rule, where we only have 1 predictor per 10 samples in the outcome of interest class (in our case 620 patients who died = 62 predictors/prinicipal components). \n",
        "\n",
        "Logistic Regression steps were followed from the following tutorial:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wpXMK_w7Kp6",
        "colab_type": "text"
      },
      "source": [
        "**Select the top 62 Prinicipal Components**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ug7VbIL7eI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_62 = X_train.iloc[:,0:62]\n",
        "X_test_62 = X_test.iloc[:,0:62]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BdqKQJ2x0em",
        "colab_type": "text"
      },
      "source": [
        "**Define and Run the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u4X6MFSxzox",
        "colab_type": "code",
        "outputId": "649a68ef-0851-4d88-d52f-cb32d76f7d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Instantiate the model (using default parameters)\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train_62, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_test_62)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRDRV7DIyGL5",
        "colab_type": "text"
      },
      "source": [
        "**Model Evaluation Using Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyJQT7fHyLeP",
        "colab_type": "code",
        "outputId": "6bb165df-da56-46aa-a26b-5c7fee2aa879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[230,  57],\n",
              "       [100,  50]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CRpg41RySQT",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix generated above is in the form of an array. Diagnosal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. The diagonal starting with the top left to the bottom right hand corner are actual predidictions, while the bottom left corner to the top right corner are incorrect predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2BvUFrAy04x",
        "colab_type": "code",
        "outputId": "150c474f-35d3-4e75-d395-9126218017ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "print(\"Accuracy:\",round(metrics.accuracy_score(y_test, y_pred),3))\n",
        "print(\"Precision:\",round(metrics.precision_score(y_test, y_pred),3))\n",
        "print(\"Recall:\",round(metrics.recall_score(y_test, y_pred),3))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.641\n",
            "Precision: 0.467\n",
            "Recall: 0.333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAZw8J4mzxsD",
        "colab_type": "text"
      },
      "source": [
        "**ROC**\n",
        "\n",
        "The Reciever Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y11_Tmpu39qG",
        "colab_type": "code",
        "outputId": "1ebfaf71-2b3c-48cd-8fd1-a015a039fc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "y_pred_proba = logreg.predict_proba(X_test_62)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, label = \"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "print('AUC:', round(auc,3))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZGA+F8ZYF734lYyqgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtFhbb0j8Gw2",
        "colab_type": "text"
      },
      "source": [
        "Fairly low sensitivity (Recall) = 33%; but the accuracy is fairly impressive that we can predict survivial just using 62 prinicipal components. This actually has better recall then the logistic regression performed with just clinical data. Additionally, the AUCs were both 67%. **Note: Keep in mind that the clinical data results I'm comparing it too, were used with a different testing/training split. \n",
        "\n",
        "I wll have re-run the neural network on this data to properly assess  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDxgR6ZhBYcS",
        "colab_type": "text"
      },
      "source": [
        "**How much of the variance do the top 62 prinicipal components contribute?**\n",
        "\n",
        "From the chart below, we can see that pc62 contribute up to ~41% of the variance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQVYsxWuBX55",
        "colab_type": "code",
        "outputId": "206ffd3b-c38e-4379-86b1-10f1b160c4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# import table of variances\n",
        "var_chart = pd.read_csv('/content/drive/My Drive/Projects/Breast_Cancer_Classification/Processed_Data/component_variance_chart_90.txt', sep=',', index_col=0)\n",
        "var_chart[:62] "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>% Variance</th>\n",
              "      <th>CumulativeVariance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pc1</th>\n",
              "      <td>6.5</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pc2</th>\n",
              "      <td>4.4</td>\n",
              "      <td>10.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pc3</th>\n",
              "      <td>3.2</td>\n",
              "      <td>14.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pc4</th>\n",
              "      <td>2.4</td>\n",
              "      <td>16.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pc5</th>\n",
              "      <td>2.1</td>\n",
              "      <td>18.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pc58</th>\n",
              "      <td>0.2</td>\n",
              "      <td>40.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pc59</th>\n",
              "      <td>0.2</td>\n",
              "      <td>40.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pc60</th>\n",
              "      <td>0.2</td>\n",
              "      <td>40.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pc61</th>\n",
              "      <td>0.2</td>\n",
              "      <td>40.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pc62</th>\n",
              "      <td>0.2</td>\n",
              "      <td>40.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      % Variance  CumulativeVariance\n",
              "pc1          6.5                 6.5\n",
              "pc2          4.4                10.9\n",
              "pc3          3.2                14.1\n",
              "pc4          2.4                16.5\n",
              "pc5          2.1                18.6\n",
              "...          ...                 ...\n",
              "pc58         0.2                40.1\n",
              "pc59         0.2                40.3\n",
              "pc60         0.2                40.5\n",
              "pc61         0.2                40.7\n",
              "pc62         0.2                40.9\n",
              "\n",
              "[62 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUCud2aO8hd6",
        "colab_type": "text"
      },
      "source": [
        "# **Assess increasingly lower numbers of Principal Components**\n",
        "\n",
        "Identify the number of principal components to represent 35, 30, 25, and 20% of varaince, and compare performance with the top 62 principal components. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSzJuaStD9Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pc_35 = var_chart[var_chart.CumulativeVariance <= 35] # 33 components\n",
        "\n",
        "pc_30 = var_chart[var_chart.CumulativeVariance <= 30] # 17 components\n",
        "\n",
        "pc_25 = var_chart[var_chart.CumulativeVariance <= 25] # 9 components\n",
        "\n",
        "pc_20 = var_chart[var_chart.CumulativeVariance <= 20] # 5 components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgrkAyTNH7rx",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression - PC 35% of variance**\n",
        "\n",
        "33 prinicipal components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b25003f6-dd8b-41b5-9a51-e33cb30da349",
        "id": "-Ym2h5aVIgRe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# Split the training/testing sets\n",
        "X_train_33 = X_train.iloc[:,0:33]\n",
        "X_test_33 = X_test.iloc[:,0:33]\n",
        "\n",
        "## Define and run the model\n",
        "# Instantiate the model (using default parameters)\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train_33, y_train)\n",
        "y_pred = logreg.predict(X_test_33)\n",
        "\n",
        "## Evaluate model \n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix', cnf_matrix)\n",
        "\n",
        "# Assess accuracy, precision, recall\n",
        "print(\"Accuracy:\",round(metrics.accuracy_score(y_test, y_pred),3))\n",
        "print(\"Precision:\",round(metrics.precision_score(y_test, y_pred),3))\n",
        "print(\"Recall:\",round(metrics.recall_score(y_test, y_pred),3))\n",
        "\n",
        "# Calculate ROC\n",
        "y_pred_proba = logreg.predict_proba(X_test_33)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, label = \"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "print('AUC:', round(auc,3))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix [[239  48]\n",
            " [100  50]]\n",
            "Accuracy: 0.661\n",
            "Precision: 0.51\n",
            "Recall: 0.333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSEAooIuIBIQCuohIQCigi4gExP8HANpWWUXmRP0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Xc3ix8hKxCB"
      },
      "source": [
        "**Logistic Regression - PC 30% of variance**\n",
        "\n",
        "17 prinicipal components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N0a5kYmDKxCC",
        "outputId": "0b8d6fb8-f17b-4962-be8e-57272c79d761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# Split the training/testing sets\n",
        "X_train_17 = X_train.iloc[:,0:17]\n",
        "X_test_17 = X_test.iloc[:,0:17]\n",
        "\n",
        "## Define and run the model\n",
        "# Instantiate the model (using default parameters)\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train_17, y_train)\n",
        "y_pred = logreg.predict(X_test_17)\n",
        "\n",
        "## Evaluate model \n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix', cnf_matrix)\n",
        "\n",
        "# Assess accuracy, precision, recall\n",
        "print(\"Accuracy:\",round(metrics.accuracy_score(y_test, y_pred),3))\n",
        "print(\"Precision:\",round(metrics.precision_score(y_test, y_pred),3))\n",
        "print(\"Recall:\",round(metrics.recall_score(y_test, y_pred),3))\n",
        "\n",
        "# Calculate ROC\n",
        "y_pred_proba = logreg.predict_proba(X_test_17)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, label = \"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "print('AUC:', round(auc,3))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix [[246  41]\n",
            " [108  42]]\n",
            "Accuracy: 0.659\n",
            "Precision: 0.506\n",
            "Recall: 0.28\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yme-r1nRLH05"
      },
      "source": [
        "**Logistic Regression - PC 25% of variance**\n",
        "\n",
        "9 prinicipal components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fK-cwbmyLH06",
        "outputId": "4833733f-25bd-49da-e58f-2c65333d8521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# Split the training/testing sets\n",
        "X_train_9 = X_train.iloc[:,0:9]\n",
        "X_test_9 = X_test.iloc[:,0:9]\n",
        "\n",
        "## Define and run the model\n",
        "# Instantiate the model (using default parameters)\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train_9, y_train)\n",
        "y_pred = logreg.predict(X_test_9)\n",
        "\n",
        "## Evaluate model \n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix', cnf_matrix)\n",
        "\n",
        "# Assess accuracy, precision, recall\n",
        "print(\"Accuracy:\",round(metrics.accuracy_score(y_test, y_pred),3))\n",
        "print(\"Precision:\",round(metrics.precision_score(y_test, y_pred),3))\n",
        "print(\"Recall:\",round(metrics.recall_score(y_test, y_pred),3))\n",
        "\n",
        "# Calculate ROC\n",
        "y_pred_proba = logreg.predict_proba(X_test_9)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, label = \"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "print('AUC:', round(auc,3))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix [[266  21]\n",
            " [129  21]]\n",
            "Accuracy: 0.657\n",
            "Precision: 0.5\n",
            "Recall: 0.14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMwVNFcOLYag"
      },
      "source": [
        "**Logistic Regression - PC 20% of variance**\n",
        "\n",
        "5 prinicipal components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ooCofl_LYai",
        "outputId": "5d389a08-0bf1-41b7-e8df-7a531c4c1ef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "# Split the training/testing sets\n",
        "X_train_5 = X_train.iloc[:,0:5]\n",
        "X_test_5 = X_test.iloc[:,0:5]\n",
        "\n",
        "## Define and run the model\n",
        "# Instantiate the model (using default parameters)\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train_5, y_train)\n",
        "y_pred = logreg.predict(X_test_5)\n",
        "\n",
        "## Evaluate model \n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix', cnf_matrix)\n",
        "\n",
        "# Assess accuracy, precision, recall\n",
        "print(\"Accuracy:\",round(metrics.accuracy_score(y_test, y_pred),3))\n",
        "print(\"Precision:\",round(metrics.precision_score(y_test, y_pred),3))\n",
        "print(\"Recall:\",round(metrics.recall_score(y_test, y_pred),3))\n",
        "\n",
        "# Calculate ROC\n",
        "y_pred_proba = logreg.predict_proba(X_test_5)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, label = \"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "print('AUC:', round(auc,3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix [[272  15]\n",
            " [138  12]]\n",
            "Accuracy: 0.65\n",
            "Precision: 0.444\n",
            "Recall: 0.08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCAU6CIiAaFAFxEJCAW6iEhA/C+1rTdSwfBRJAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hOSXu7mJv7P",
        "colab_type": "text"
      },
      "source": [
        "# **Logistic Regression Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG_M72mWIxrQ",
        "colab_type": "text"
      },
      "source": [
        "**Table of Logistic Regression Results**\n",
        "\n",
        "**62 Prinicipal Components (41%) of the variance**\n",
        "* Accuracy: 0.641\n",
        "* Precision: 0.467\n",
        "* Recall: 0.333\n",
        "* AUC: 0.665\n",
        "\n",
        "**33 Prinicipal Components (35%) of the variance**\n",
        "* Accuracy: 0.661\n",
        "* Precision: 0.51\n",
        "* Recall: 0.333\n",
        "* AUC: 0.660\n",
        "\n",
        "**17 Prinicipal Components (30%) of the variance**\n",
        "* Accuracy: 0.659\n",
        "* Precision: 0.506\n",
        "* Recall: 0.28\n",
        "* AUC: 0.649\n",
        "\n",
        "**9 Prinicipal Components (25%) of the variance**\n",
        "* Accuracy: 0.657\n",
        "* Precision: 0.5\n",
        "* Recall: 0.14\n",
        "* AUC: 0.628\n",
        "\n",
        "**5 Prinicipal Components (20%) of the variance**\n",
        "* Accuracy: 0.65\n",
        "* Precision: 0.444\n",
        "* Recall: 0.08\n",
        "* AUC: 0.625\n",
        "\n",
        "As we can see, recall and AUC continue to decrease when we test under 33 prinicipal components. Intersetingly, 33 prinicipal components seems to perform simiarly to 62 prinicipal components. For instance, recall is the same, and the AUC is only slightly lower with 33 componenets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M5jCMHP8COG",
        "colab_type": "text"
      },
      "source": [
        "# **Random Forest Classification**\n",
        "\n",
        "We assess the performance of a random forest classifier using 1213 prinicipal components (those found responsible for 90% of the variance), 62 and 33 prinicipal components. \n",
        "\n",
        "**Is there an upper limit on the number of predictors in a random forest model?**\n",
        "\n",
        "References:  https://towardsdatascience.com/random-forest-in-python-24d0893d51c0 and https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BexDfa_OUM5S",
        "colab_type": "text"
      },
      "source": [
        "**1213 Prinicipal Components (90% of the Variance)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C_0u1EQ7UjhG"
      },
      "source": [
        "**Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n0L2WxhQUjhJ",
        "colab": {}
      },
      "source": [
        "# Instantiate model with 1000 decision trees\n",
        "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
        "\n",
        "# Train the model on training data\n",
        "rf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1xFMAhmHUjhM"
      },
      "source": [
        "**Select Important Features**\n",
        "\n",
        "Identify which Principal Components contributed most to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VhTwF1NQUjhM",
        "colab": {}
      },
      "source": [
        "# First feature selection test\n",
        "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
        "sel.fit(X_train, y_train)\n",
        "sel.get_support()\n",
        "selected_feat= X_train.columns[(sel.get_support())]\n",
        "print(len(selected_feat)) \n",
        "print(selected_feat)\n",
        "\n",
        "## histogram of the top 10 pc by importance\n",
        "# create a list of features/column names\n",
        "features = list(X_train.columns.values) \n",
        "\n",
        "feat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)  \n",
        "feat_importances.nlargest(10).plot(kind='barh')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sg56uv8LUjhQ"
      },
      "source": [
        "** **I should ask about this: The finding above is intersting because though PC1 is said to explain the most variance, it is not even in the plot of the top 10 prinicipal components the random forest found to be of importance. Though PC2 is implicated as being the most important** **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_GDSaXpUjhQ"
      },
      "source": [
        "**Make Predictions on the Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UayB7jKrUjhR",
        "colab": {}
      },
      "source": [
        "# Use the forest's predict method on the test data\n",
        "predictions = rf.predict(X_test)\n",
        "# Probabilities for each class\n",
        "rf_probs = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the absolute errors\n",
        "errors = abs(predictions - y_test)\n",
        "# Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oT2PrzJAUjhX"
      },
      "source": [
        "**Evaluating the Performance**\n",
        "\n",
        "For Classification Problems the metrics used to evaluate an algorithm are accuracy, confusion matrix, precision, recall, and F1 values.\n",
        "\n",
        "We can also perform cross-fold validation to have a better understanding of the results\n",
        "\n",
        "Reference: https://medium.com/@hjhuney/implementing-a-random-forest-classification-model-in-python-583891c99652 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oD8KtZYuUjhX"
      },
      "source": [
        "**ROC on the Full Data**\n",
        "\n",
        "\n",
        "We make class predictions (predict) as well as predicted probabilities (predict_proba) to calculate the ROC AUC. Once we have the testing predictions, we can calculate the ROC AUC. https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qmvgdzrCUjhb",
        "colab": {}
      },
      "source": [
        "# Calculate ROC AUC\n",
        "roc_value = roc_auc_score(y_test, rf_probs) \n",
        "roc_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RGebJ4VZUjhd",
        "colab": {}
      },
      "source": [
        "# Probabilities for each class\n",
        "# Note from scikit learn: re fpr and tpr: \"Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both fpr and tpr, which are sorted in reversed order during their calculation.\"\n",
        "rf_probs = rf.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, rf_probs)\n",
        "auc = metrics.roc_auc_score(y_test, rf_probs)\n",
        "auc = round(auc, 4)\n",
        "plt.plot(fpr, tpr, label=\"date 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C-2FmyyUUjhf"
      },
      "source": [
        "**10 Cross-Fold Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rH47sFYAUjhi",
        "colab": {}
      },
      "source": [
        "rfc_cv_score = cross_val_score(rf, x, y, cv=10, scoring='roc_auc') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gv6CioPzUjhk"
      },
      "source": [
        "Below, we also can examine the confusion matrix, the classification report containing precision, recall, f1-score, and support, All AUC scores, and the mean AUC score\n",
        "\n",
        "The **confusion matrix** is useful for giving false positives and false negatives\n",
        "\n",
        "The **ROC Curve** plots out the true positive rate vs the false positive rate at various thresholds\n",
        "\n",
        "The **ROC AUC Scoring** used in the cross-validation model shows the area under the ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yndd2vRpUjhk",
        "colab": {}
      },
      "source": [
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, predictions))\n",
        "print('\\n')\n",
        "print(\"=== All AUC Scores ===\")\n",
        "print(rfc_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean AUC Score ===\")\n",
        "print(\"Mean AUC Score - Random Forest: \", rfc_cv_score.mean())\n",
        "print('\\n')\n",
        "print(\"=== Accuracy Score ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJD32vZ8bQOb",
        "colab_type": "text"
      },
      "source": [
        "It looks like 1213 PC may be too many for us to analyze -- we get a recall of 0 -- this actually makes me wonder if I messed something up with the code? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itWBnwaGUHSF",
        "colab_type": "text"
      },
      "source": [
        "**62 Prinicipal Components (41% of the Variance)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpN91JbL9JOA",
        "colab_type": "text"
      },
      "source": [
        "**Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dsBsgBv9LWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate model with 1000 decision trees - results below reported w/ random state = 42\n",
        "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
        "\n",
        "# Train the model on training data\n",
        "rf.fit(X_train_62, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZbwAmNQ-XdF",
        "colab_type": "text"
      },
      "source": [
        "**Select Important Features**\n",
        "\n",
        "Identify which Principal Components contributed most to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_MdbNeW-bHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First feature selection test\n",
        "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
        "sel.fit(X_train_62, y_train)\n",
        "sel.get_support()\n",
        "selected_feat= X_train_62.columns[(sel.get_support())]\n",
        "print(len(selected_feat)) \n",
        "print(selected_feat)\n",
        "\n",
        "## histogram of the top 10 pc by importance\n",
        "# create a list of features/column names\n",
        "features = list(X_train_62.columns.values) \n",
        "\n",
        "feat_importances = pd.Series(rf.feature_importances_, index=X_train_62.columns)  \n",
        "feat_importances.nlargest(10).plot(kind='barh')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFUXfVprIEVB",
        "colab_type": "text"
      },
      "source": [
        "** **I should ask about this: The finding above is intersting because though PC1 is said to explain the most variance, it is not even in the plot of the top 10 prinicipal components the random forest found to be of importance. Though PC2 and PC3 reside in the top 10!** **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FENSsY5l9aDT",
        "colab_type": "text"
      },
      "source": [
        "**Make Predictions on the Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yKF_xdj9dbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the forest's predict method on the test data\n",
        "predictions = rf.predict(X_test_62)\n",
        "# Probabilities for each class\n",
        "rf_probs = rf.predict_proba(X_test_62)[:, 1]\n",
        "\n",
        "# Calculate the absolute errors\n",
        "errors = abs(predictions - y_test)\n",
        "# Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5pOirYXDrEn",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the Performance**\n",
        "\n",
        "For Classification Problems the metrics used to evaluate an algorithm are accuracy, confusion matrix, precision, recall, and F1 values.\n",
        "\n",
        "We can also perform cross-fold validation to have a better understanding of the results\n",
        "\n",
        "Reference: https://medium.com/@hjhuney/implementing-a-random-forest-classification-model-in-python-583891c99652 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu4eohUxFVaA",
        "colab_type": "text"
      },
      "source": [
        "**ROC on the Full Data**\n",
        "\n",
        "\n",
        "We make class predictions (predict) as well as predicted probabilities (predict_proba) to calculate the ROC AUC. Once we have the testing predictions, we can calculate the ROC AUC. https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrO0-IuE-o0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate ROC AUC\n",
        "roc_value = roc_auc_score(y_test, rf_probs) \n",
        "roc_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyKejE-yF88f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Probabilities for each class\n",
        "# Note from scikit learn: re fpr and tpr: \"Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both fpr and tpr, which are sorted in reversed order during their calculation.\"\n",
        "rf_probs = rf.predict_proba(X_test_62)[:, 1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, rf_probs)\n",
        "auc = metrics.roc_auc_score(y_test, rf_probs)\n",
        "auc = round(auc, 4)\n",
        "plt.plot(fpr, tpr, label=\"date 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U0IYU-uFzGZ",
        "colab_type": "text"
      },
      "source": [
        "**10 Cross-Fold Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQRXv1rIE9UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to combine the testing and training sets to perform cross-fold varlidation\n",
        "# select the 62 pc columns\n",
        "x_62 = x.iloc[:,0:62]\n",
        "y_62 = y\n",
        "\n",
        "rfc_cv_score = cross_val_score(rf, x_62, y_62, cv=10, scoring='roc_auc') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEonYh2WEhkU",
        "colab_type": "text"
      },
      "source": [
        "Below, we also can examine the confusion matrix, the classification report containing precision, recall, f1-score, and support, All AUC scores, and the mean AUC score\n",
        "\n",
        "The **confusion matrix** is useful for giving false positives and false negatives\n",
        "\n",
        "The **ROC Curve** plots out the true positive rate vs the false positive rate at various thresholds\n",
        "\n",
        "The **ROC AUC Scoring** used in the cross-validation model shows the area under the ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haZRHJqGcv9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, predictions))\n",
        "print('\\n')\n",
        "print(\"=== All AUC Scores ===\")\n",
        "print(rfc_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean AUC Score ===\")\n",
        "print(\"Mean AUC Score - Random Forest: \", rfc_cv_score.mean())\n",
        "print('\\n')\n",
        "print(\"=== Accuracy Score ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Rt_2XZUZ7N",
        "colab_type": "text"
      },
      "source": [
        "**33 Prinicipal Components (35% of the Variance)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FzmD72vGXGeR"
      },
      "source": [
        "**Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V4BekCdMXGef",
        "colab": {}
      },
      "source": [
        "# Instantiate model with 1000 decision trees\n",
        "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42) # results below reported w/ random state = 42\n",
        "\n",
        "# Train the model on training data\n",
        "rf.fit(X_train_33, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cOom3lxOXGer"
      },
      "source": [
        "**Select Important Features**\n",
        "\n",
        "Identify which Principal Components contributed most to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D0xAufOyXGes",
        "colab": {}
      },
      "source": [
        "# First feature selection test\n",
        "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
        "sel.fit(X_train_33, y_train)\n",
        "sel.get_support()\n",
        "selected_feat= X_train_33.columns[(sel.get_support())]\n",
        "print(len(selected_feat)) \n",
        "print(selected_feat)\n",
        "\n",
        "## histogram of the top 10 pc by importance\n",
        "# create a list of features/column names\n",
        "features = list(X_train_33.columns.values) \n",
        "\n",
        "feat_importances = pd.Series(rf.feature_importances_, index=X_train_33.columns)  \n",
        "feat_importances.nlargest(10).plot(kind='barh')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ycKH-E5QXGe2"
      },
      "source": [
        "** **I should ask about this: The finding above is intersting because though PC1 is said to explain the most variance, it is not even in the plot of the top 10 prinicipal components the random forest found to be of importance. PC2 and PC3 were found in the top 10 in this test.** **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IEkp4XAdXGe8"
      },
      "source": [
        "**Make Predictions on the Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEx7u0msXGe_",
        "colab": {}
      },
      "source": [
        "# Use the forest's predict method on the test data\n",
        "predictions = rf.predict(X_test_33)\n",
        "# Probabilities for each class\n",
        "rf_probs = rf.predict_proba(X_test_33)[:, 1]\n",
        "\n",
        "# Calculate the absolute errors\n",
        "errors = abs(predictions - y_test)\n",
        "# Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-5PL9FdiXGfK"
      },
      "source": [
        "**Evaluating the Performance**\n",
        "\n",
        "For Classification Problems the metrics used to evaluate an algorithm are accuracy, confusion matrix, precision, recall, and F1 values.\n",
        "\n",
        "We can also perform cross-fold validation to have a better understanding of the results\n",
        "\n",
        "Reference: https://medium.com/@hjhuney/implementing-a-random-forest-classification-model-in-python-583891c99652 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1BO07lHLXGfM"
      },
      "source": [
        "**ROC on the Full Data**\n",
        "\n",
        "\n",
        "We make class predictions (predict) as well as predicted probabilities (predict_proba) to calculate the ROC AUC. Once we have the testing predictions, we can calculate the ROC AUC. https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zrQY6TRbXGfZ",
        "colab": {}
      },
      "source": [
        "# Calculate ROC AUC\n",
        "roc_value = roc_auc_score(y_test, rf_probs) \n",
        "roc_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Yr6tOxNXGfg",
        "colab": {}
      },
      "source": [
        "# Probabilities for each class\n",
        "# Note from scikit learn: re fpr and tpr: \"Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both fpr and tpr, which are sorted in reversed order during their calculation.\"\n",
        "rf_probs = rf.predict_proba(X_test_33)[:, 1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, rf_probs)\n",
        "auc = metrics.roc_auc_score(y_test, rf_probs)\n",
        "auc = round(auc, 4)\n",
        "plt.plot(fpr, tpr, label=\"date 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bEoQWOGAXGfl"
      },
      "source": [
        "**10 Cross-Fold Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hJSLBu04XGfm",
        "colab": {}
      },
      "source": [
        "# need to combine the testing and training sets to perform cross-fold varlidation\n",
        "# select the 33 pc columns\n",
        "x_33 = x.iloc[:,0:33]\n",
        "y_33 = y\n",
        "\n",
        "rfc_cv_score = cross_val_score(rf, x_33, y_33, cv=10, scoring='roc_auc') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vZwd1wqnXGfq"
      },
      "source": [
        "Below, we also can examine the confusion matrix, the classification report containing precision, recall, f1-score, and support, All AUC scores, and the mean AUC score\n",
        "\n",
        "The **confusion matrix** is useful for giving false positives and false negatives\n",
        "\n",
        "The **ROC Curve** plots out the true positive rate vs the false positive rate at various thresholds\n",
        "\n",
        "The **ROC AUC Scoring** used in the cross-validation model shows the area under the ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tpYKGm-qXGfr",
        "colab": {}
      },
      "source": [
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, predictions))\n",
        "print('\\n')\n",
        "print(\"=== All AUC Scores ===\")\n",
        "print(rfc_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean AUC Score ===\")\n",
        "print(\"Mean AUC Score - Random Forest: \", rfc_cv_score.mean())\n",
        "print('\\n')\n",
        "print(\"=== Accuracy Score ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DjwC19GSX8j",
        "colab_type": "text"
      },
      "source": [
        "# **Random Forest Results**\n",
        "\n",
        "**Table of Random Forest Results**\n",
        "\n",
        "**1213 Prinicipal Components (90%) of the variance**\n",
        "\n",
        "I am not going to include these results -- Recall was 0\n",
        "\n",
        "**62 Prinicipal Components (41%) of the variance**\n",
        "\n",
        "* AUC: 0.637\n",
        "* Mean Absolute Error: 0.36\n",
        "* Accuracy: 0.643\n",
        "* Mean AUC from 10 CFV: 0.62\n",
        "                   === Classification Report ===\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.66      0.93      0.77       287\n",
        "         1.0       0.41      0.09      0.15       150\n",
        "       accuracy                        0.64       437\n",
        "       macro avg   0.54      0.51      0.46       437\n",
        "      weighted avg 0.58      0.64      0.56       437 \n",
        "\n",
        "Sensitivity is very low with this method.\n",
        "\n",
        "**33 Prinicipal Components (35%) of the variance**\n",
        "* AUC: 0.623\n",
        "* Mean Absolute Error: 0.33\n",
        "* Accuracy: 0.67\n",
        "* Mean AUC from 10 CFV: 0.67\n",
        "\n",
        "                    === Classification Report ===\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         0.0       0.69      0.91      0.79       287\n",
        "         1.0       0.56      0.21      0.31       150\n",
        "       accuracy                        0.67       437\n",
        "       macro avg   0.63      0.56      0.55       437\n",
        "      weighted avg 0.65      0.67      0.62       437\n",
        "\n",
        "\n",
        "Overall, it does not look like we can expect good results with all 1213 Prinicipal Components. Additionally, AUCs and recall were better in the logistic regression models. \n",
        "\n",
        "Suprisingly, Recall was higher when testing 33 prinicipal components with a random forest classifier. I wonder if may that was just by chance.\n",
        "\n",
        "**Note:** I changed the random see for pc_33 random forest results because I wanted to assess whether or not we'd get the same recall. Thus, the actual results may be slightly different from what was reported in this results chunk. **Update:** Recall is still 0.17. \n",
        "\n",
        "I also tried changing random number see for 62 prinicpal components, and the recall was .10 ~~ so looks like perhaps 33 prinicipal components performs better with random forests -- although more tests would help ensure this. \n",
        "\n",
        "**One thing to maybe look into is changing the number of trees. I wonder if this would improve results at all.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_cKdGjWcZkl",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Network Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhs-KCueHlKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PyTorch packages\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_curve, auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPiLg0DgkuU",
        "colab_type": "text"
      },
      "source": [
        "**Assess Neural Network Classification Using all 1213 Prinicipal Components**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WMA1F7XxV8a",
        "colab_type": "text"
      },
      "source": [
        "**Format the Training Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7pj4XyVzUIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert data into arrays\n",
        "xb = np.array(X_train, dtype = \"float32\")\n",
        "yb = np.array(y_train, dtype = \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "# Combine the arrays\n",
        "trainloader = TensorDataset(xb, yb)\n",
        "\n",
        "# Define the batchsize\n",
        "batch_size = 32\n",
        "\n",
        "# Training Loader\n",
        "trainloader = DataLoader(trainloader, batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtV6u9fUzrxx",
        "colab_type": "text"
      },
      "source": [
        "**Format the Testing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xoH_jHfzwc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert data into arrays\n",
        "xb = np.array(X_test, dtype = \"float32\")\n",
        "yb = np.array(y_test, dtype = \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "# Combine the arrays\n",
        "testloader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size= 32\n",
        "\n",
        "# Training Loader\n",
        "testloader = DataLoader(testloader, batch_size, shuffle=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOzyQUmc0PJ6",
        "colab_type": "text"
      },
      "source": [
        "**Create Neural Network Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2XAPEny0Thm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ba506d6-b125-4a35-9e4d-0d8f1408f69a"
      },
      "source": [
        "# Define the model with hidden layers - 1213 inputs\n",
        "model = nn.Sequential(nn.Linear(1213, 500),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(500, 250),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(250, 1), \n",
        "                      nn.Sigmoid())\n",
        "                      \n",
        "# Set optimizer and learning rate\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "\n",
        "# Could also use Adam optimizer; similar to stochastic gradient descent, but uses momentum which can speed up the actual fitting process, and it also adjusts the learning rate for each of the individual parameters in the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "criterion = nn.BCELoss() # use with a sigmoid function\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss() #don't use with softmax or sigmoid\n",
        "\n",
        "# Set epochs\n",
        "epochs = 200\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for xb, yb in trainloader:\n",
        "        \n",
        "        # Clear the gradients, do this because gradients are accumulated\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Training pass\n",
        "        output = model.forward(xb)\n",
        "        loss = criterion(output, yb) # Loss calculated from the output compared to the labels  \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() # loss.item() gets the scalar value held in the loss. Running_loss = 0, \n",
        "        # += notation, says \"Add a value and the variable and assigns the result to that variable.\" So, adds the running_loss (0) with loss.item and assigns to running_loss\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.6565079630875006\n",
            "Training loss: 0.48867770738717986\n",
            "Training loss: 0.3614494698803599\n",
            "Training loss: 0.23270732818580256\n",
            "Training loss: 0.1290956973666098\n",
            "Training loss: 0.06472030899873595\n",
            "Training loss: 0.03330829544220029\n",
            "Training loss: 0.019279085349564146\n",
            "Training loss: 0.012364612165384176\n",
            "Training loss: 0.00862605898164031\n",
            "Training loss: 0.00635150212385669\n",
            "Training loss: 0.004878085015750513\n",
            "Training loss: 0.0038623722190627963\n",
            "Training loss: 0.0031311455148658376\n",
            "Training loss: 0.00259436879352462\n",
            "Training loss: 0.0021842839820991927\n",
            "Training loss: 0.0018592974903607151\n",
            "Training loss: 0.0016025225830650547\n",
            "Training loss: 0.0013949043719415984\n",
            "Training loss: 0.001225382629100506\n",
            "Training loss: 0.0010840147449748545\n",
            "Training loss: 0.0009667907029435765\n",
            "Training loss: 0.0008643004906977095\n",
            "Training loss: 0.0007784390217261162\n",
            "Training loss: 0.0007030339989663534\n",
            "Training loss: 0.0006386738649580809\n",
            "Training loss: 0.000582580512855202\n",
            "Training loss: 0.0005329178906011781\n",
            "Training loss: 0.000489184421362219\n",
            "Training loss: 0.00045015142943778235\n",
            "Training loss: 0.0004156140181826564\n",
            "Training loss: 0.00038445118329169726\n",
            "Training loss: 0.00035660012650712416\n",
            "Training loss: 0.00033139100223041437\n",
            "Training loss: 0.00030884312653187205\n",
            "Training loss: 0.0002882983336770316\n",
            "Training loss: 0.000269310437100258\n",
            "Training loss: 0.0002524404362037142\n",
            "Training loss: 0.0002368351908843601\n",
            "Training loss: 0.0002223132217229094\n",
            "Training loss: 0.00020915344527854424\n",
            "Training loss: 0.00019695931973174306\n",
            "Training loss: 0.00018570589351683584\n",
            "Training loss: 0.00017529281396447204\n",
            "Training loss: 0.00016575362927297402\n",
            "Training loss: 0.00015685635831110497\n",
            "Training loss: 0.00014855356236760753\n",
            "Training loss: 0.00014094360529093026\n",
            "Training loss: 0.0001337622668914974\n",
            "Training loss: 0.00012702432375191142\n",
            "Training loss: 0.00012072620834593048\n",
            "Training loss: 0.00011487971251637379\n",
            "Training loss: 0.00010941143409581855\n",
            "Training loss: 0.00010424846124540015\n",
            "Training loss: 9.93997392226181e-05\n",
            "Training loss: 9.483175310334645e-05\n",
            "Training loss: 9.054997564994776e-05\n",
            "Training loss: 8.65221353284674e-05\n",
            "Training loss: 8.263521255973568e-05\n",
            "Training loss: 7.90547795676081e-05\n",
            "Training loss: 7.571201138311951e-05\n",
            "Training loss: 7.246200455971652e-05\n",
            "Training loss: 6.945977976301504e-05\n",
            "Training loss: 6.655594383695776e-05\n",
            "Training loss: 6.381480311463791e-05\n",
            "Training loss: 6.121891135812691e-05\n",
            "Training loss: 5.875649580181145e-05\n",
            "Training loss: 5.6452369768166825e-05\n",
            "Training loss: 5.4227287431031736e-05\n",
            "Training loss: 5.2059665726918e-05\n",
            "Training loss: 5.0076753701077684e-05\n",
            "Training loss: 4.818694749080417e-05\n",
            "Training loss: 4.634968877880743e-05\n",
            "Training loss: 4.4606285882764496e-05\n",
            "Training loss: 4.294164469550795e-05\n",
            "Training loss: 4.13439793192083e-05\n",
            "Training loss: 3.984905188411997e-05\n",
            "Training loss: 3.8383141899674494e-05\n",
            "Training loss: 3.699725382874731e-05\n",
            "Training loss: 3.569793887258927e-05\n",
            "Training loss: 3.4428055209346206e-05\n",
            "Training loss: 3.321955853430043e-05\n",
            "Training loss: 3.2068521826792076e-05\n",
            "Training loss: 3.095501863465804e-05\n",
            "Training loss: 2.986832761252583e-05\n",
            "Training loss: 2.8861900873158536e-05\n",
            "Training loss: 2.7851085089822868e-05\n",
            "Training loss: 2.6915679283286784e-05\n",
            "Training loss: 2.6002036176533892e-05\n",
            "Training loss: 2.5143464704231563e-05\n",
            "Training loss: 2.4304711432040032e-05\n",
            "Training loss: 2.3508598155231305e-05\n",
            "Training loss: 2.2744376021763878e-05\n",
            "Training loss: 2.199591991240329e-05\n",
            "Training loss: 2.1295293086817455e-05\n",
            "Training loss: 2.060963173035572e-05\n",
            "Training loss: 1.9948184893930772e-05\n",
            "Training loss: 1.9303436136219605e-05\n",
            "Training loss: 1.8703269255993512e-05\n",
            "Training loss: 1.8102000977683575e-05\n",
            "Training loss: 1.7535554700482796e-05\n",
            "Training loss: 1.6986427997436272e-05\n",
            "Training loss: 1.645404857454861e-05\n",
            "Training loss: 1.5953662091860458e-05\n",
            "Training loss: 1.5455190539175485e-05\n",
            "Training loss: 1.4989942229010745e-05\n",
            "Training loss: 1.4531527374013576e-05\n",
            "Training loss: 1.4090362195175413e-05\n",
            "Training loss: 1.3666014704695709e-05\n",
            "Training loss: 1.3249625803060864e-05\n",
            "Training loss: 1.285752147473764e-05\n",
            "Training loss: 1.2474385917679372e-05\n",
            "Training loss: 1.20941179530502e-05\n",
            "Training loss: 1.1746342951061329e-05\n",
            "Training loss: 1.1393480291167734e-05\n",
            "Training loss: 1.1061934279363102e-05\n",
            "Training loss: 1.0733171964647028e-05\n",
            "Training loss: 1.0420316612442371e-05\n",
            "Training loss: 1.0113507081902155e-05\n",
            "Training loss: 9.822083883613587e-06\n",
            "Training loss: 9.540661367060327e-06\n",
            "Training loss: 9.265535925628572e-06\n",
            "Training loss: 9.002928425438657e-06\n",
            "Training loss: 8.743213728427313e-06\n",
            "Training loss: 8.49330312919891e-06\n",
            "Training loss: 8.251115726058588e-06\n",
            "Training loss: 8.018211468790868e-06\n",
            "Training loss: 7.790917052483476e-06\n",
            "Training loss: 7.570906368860827e-06\n",
            "Training loss: 7.35922822059365e-06\n",
            "Training loss: 7.152701716510885e-06\n",
            "Training loss: 6.95006136128341e-06\n",
            "Training loss: 6.760301990963365e-06\n",
            "Training loss: 6.569804011122891e-06\n",
            "Training loss: 6.392241707877341e-06\n",
            "Training loss: 6.214128250842517e-06\n",
            "Training loss: 6.040766759436271e-06\n",
            "Training loss: 5.8745956998680455e-06\n",
            "Training loss: 5.720194239318782e-06\n",
            "Training loss: 5.557851570146584e-06\n",
            "Training loss: 5.408262944822526e-06\n",
            "Training loss: 5.259134557091762e-06\n",
            "Training loss: 5.118695808938e-06\n",
            "Training loss: 4.976863814723712e-06\n",
            "Training loss: 4.841158925489985e-06\n",
            "Training loss: 4.714700730246699e-06\n",
            "Training loss: 4.584950480287861e-06\n",
            "Training loss: 4.461969283289452e-06\n",
            "Training loss: 4.342662056687248e-06\n",
            "Training loss: 4.226913649578885e-06\n",
            "Training loss: 4.113294447207165e-06\n",
            "Training loss: 4.005880998924912e-06\n",
            "Training loss: 3.896968398360669e-06\n",
            "Training loss: 3.793649783602166e-06\n",
            "Training loss: 3.694641247598267e-06\n",
            "Training loss: 3.595844529582991e-06\n",
            "Training loss: 3.501945434991063e-06\n",
            "Training loss: 3.407991737442832e-06\n",
            "Training loss: 3.3202651782009593e-06\n",
            "Training loss: 3.233568233509886e-06\n",
            "Training loss: 3.1486189660737603e-06\n",
            "Training loss: 3.066244106576494e-06\n",
            "Training loss: 2.987282650016224e-06\n",
            "Training loss: 2.908575616280226e-06\n",
            "Training loss: 2.8324974686611534e-06\n",
            "Training loss: 2.757591457294638e-06\n",
            "Training loss: 2.6859413343650308e-06\n",
            "Training loss: 2.618010407010244e-06\n",
            "Training loss: 2.5501249987561406e-06\n",
            "Training loss: 2.482693927471168e-06\n",
            "Training loss: 2.4183581023485535e-06\n",
            "Training loss: 2.3579748302858563e-06\n",
            "Training loss: 2.2979519898643316e-06\n",
            "Training loss: 2.23705987215309e-06\n",
            "Training loss: 2.1796385553676462e-06\n",
            "Training loss: 2.123543958432714e-06\n",
            "Training loss: 2.0698814751085592e-06\n",
            "Training loss: 2.015625202658135e-06\n",
            "Training loss: 1.9662452636910572e-06\n",
            "Training loss: 1.9155659729000125e-06\n",
            "Training loss: 1.8667523635286205e-06\n",
            "Training loss: 1.8187625678032552e-06\n",
            "Training loss: 1.7739226877449433e-06\n",
            "Training loss: 1.7288252848163942e-06\n",
            "Training loss: 1.6849182108072876e-06\n",
            "Training loss: 1.6428676899966198e-06\n",
            "Training loss: 1.6004628904982648e-06\n",
            "Training loss: 1.5620710561362523e-06\n",
            "Training loss: 1.5212623311083186e-06\n",
            "Training loss: 1.4816287980133118e-06\n",
            "Training loss: 1.4451723184653774e-06\n",
            "Training loss: 1.4094487945973325e-06\n",
            "Training loss: 1.374954932521732e-06\n",
            "Training loss: 1.3405216599849376e-06\n",
            "Training loss: 1.3073846239754724e-06\n",
            "Training loss: 1.2730513459445785e-06\n",
            "Training loss: 1.2407865957667548e-06\n",
            "Training loss: 1.2104602463784055e-06\n",
            "Training loss: 1.1805397382043494e-06\n",
            "Training loss: 1.1511341024676612e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXT-gAEk3aiZ",
        "colab_type": "text"
      },
      "source": [
        "**Evaluate Model Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JLqRn3qcypE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "7a4d6a9d-9fa7-4dde-f9cc-378a843cb0bf"
      },
      "source": [
        "# Convert data into arrays\n",
        "xb = np.array(X_test, dtype = \"float32\")\n",
        "yb = np.array(y_test, dtype = \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "# Apply the model to the whole testing dataset\n",
        "ps = model(xb)\n",
        "\n",
        "#print('Probabilities', ps[:10])\n",
        "\n",
        "# Obtain the top probability\n",
        "top_p, top_class = ps.topk(1, dim=1)\n",
        "#print('true vals', yb[:10])\n",
        "\n",
        "# Drop the grad \n",
        "top_p = top_p.detach().numpy()\n",
        "top_class = top_class.detach().numpy()\n",
        "\n",
        "top_class = (top_p >= 0.5).astype(np.int)\n",
        "#print('top class', top_class[:10])\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(yb, top_p, pos_label=1)\n",
        "\n",
        "# Compute ROC area\n",
        "roc_auc = round(auc(fpr, tpr),3)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.3f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(yb, top_class))\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(yb, top_class))\n",
        "print('\\n')\n",
        "print(\"=== Accuracy Score ===\")\n",
        "print(\"Accuracy:\", round(accuracy_score(yb, top_class),3))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Confusion Matrix ===\n",
            "[[223  64]\n",
            " [ 87  63]]\n",
            "\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.78      0.75       287\n",
            "         1.0       0.50      0.42      0.45       150\n",
            "\n",
            "    accuracy                           0.65       437\n",
            "   macro avg       0.61      0.60      0.60       437\n",
            "weighted avg       0.64      0.65      0.65       437\n",
            "\n",
            "\n",
            "\n",
            "=== Accuracy Score ===\n",
            "Accuracy: 0.654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nicIYY4xP/w8KWdoS5ug8OQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEPXvoss2fF6",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Network Experiments**\n",
        "\n",
        "**December 3rd - 1213 Principal Components, Neural Network Architecture:**\n",
        "\n",
        "**Loss:** Binary Cross Entropy Loss\n",
        "\n",
        "**Activation:** ReLU\n",
        "\n",
        "**Batchsize:** 32\n",
        "\n",
        "1) Hidden Layers (500, 10), epochs = 50, Adam, LR = 0.001\n",
        "\n",
        "AUC = 0.62\n",
        "\n",
        "Accuracy: 0.654\n",
        "\n",
        "Recall: 0.45\n",
        "\n",
        "2) Hidden Layers (500, 10), epochs = 100, Adam, LR = 0.001\n",
        "\n",
        "AUC = 0.630\n",
        "\n",
        "Accuracy: 0.65\n",
        "\n",
        "3) Hidden Layers (500, 10), epochs = 100, Adam, LR = 0.003\n",
        "\n",
        "AUC = 0.616\n",
        "\n",
        "Accuracy: 0.618\n",
        "\n",
        "4) Hidden Layers (500, 10), epochs = 200, Adam, LR = 0.001\n",
        "\n",
        "AUC = 0.640\n",
        "\n",
        "Accuracy: 0.634\n",
        "\n",
        "5) Hidden Layers (500, 10), epochs = 200, Adam, LR = 0.0009\n",
        "\n",
        "AUC = 0.638\n",
        "\n",
        "Accuracy: 0.666\n",
        "\n",
        "6) Hidden Layers (500, 250), epochs = 200, Adam, LR = 0.001\n",
        "\n",
        "AUC = 0.623\n",
        "\n",
        "Accuracy: 0.643\n",
        "\n",
        "7) Hidden Layers (500, 250), epochs = 200, Adam, LR = 0.0001\n",
        "\n",
        "AUC = 0.648\n",
        "\n",
        "Accuracy: 0.659\n",
        "\n",
        "\n",
        "8) Hidden Layers (500, 250), epochs = 200, Adam, LR = 0.0005\n",
        "\n",
        "AUC = 0.648\n",
        "\n",
        "Accuracy: 0.641\n",
        "\n",
        "9) Hidden Layers (500, 250), epochs = 200, Adam, LR = 0.00009\n",
        "\n",
        "AUC = 0.627\n",
        "\n",
        "Accuracy: 0.654\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pDLyPK1qh6fp"
      },
      "source": [
        "**Assess Neural Network Classification Using 62 Prinicipal Components (41% of the Variance)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qJ4aFkO-h6f5"
      },
      "source": [
        "**Format the Training Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eD62jTuYh6f6",
        "colab": {}
      },
      "source": [
        "# Convert data into arrays\n",
        "xb = np.array(X_train_62, dtype = \"float32\")\n",
        "yb = np.array(y_train, dtype = \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "# Combine the arrays\n",
        "trainloader = TensorDataset(xb, yb)\n",
        "\n",
        "# Define the batchsize\n",
        "batch_size = 32\n",
        "\n",
        "# Training Loader\n",
        "trainloader = DataLoader(trainloader, batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wT85ppDZh6f_"
      },
      "source": [
        "**Format the Testing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UjBEuod6h6gA",
        "colab": {}
      },
      "source": [
        "# Convert data into arrays\n",
        "xb = np.array(X_test_62, dtype = \"float32\")\n",
        "yb = np.array(y_test, dtype = \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "# Combine the arrays\n",
        "testloader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size= 32\n",
        "\n",
        "# Training Loader\n",
        "testloader = DataLoader(testloader, batch_size, shuffle=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ouAtJO7gh6gC"
      },
      "source": [
        "**Create Neural Network Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WM7oHBkSh6gD",
        "outputId": "d709b4e3-34fc-4be6-fd7c-3831547650dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define the model with hidden layers - 62 inputs\n",
        "model = nn.Sequential(nn.Linear(62, 30),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(30, 10),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(10, 1),\n",
        "                      nn.Sigmoid())\n",
        "                      \n",
        "# Set optimizer and learning rate\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "\n",
        "# Could also use Adam optimizer; similar to stochastic gradient descent, but uses momentum which can speed up the actual fitting process, and it also adjusts the learning rate for each of the individual parameters in the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "criterion = nn.BCELoss() # use with a sigmoid function\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss() #don't use with softmax or sigmoid\n",
        "\n",
        "# Set epochs\n",
        "epochs = 200\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for xb, yb in trainloader:\n",
        "        \n",
        "        # Clear the gradients, do this because gradients are accumulated\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Training pass\n",
        "        output = model.forward(xb)\n",
        "        loss = criterion(output, yb) # Loss calculated from the output compared to the labels  \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() # loss.item() gets the scalar value held in the loss. Running_loss = 0, \n",
        "        # += notation, says \"Add a value and the variable and assigns the result to that variable.\" So, adds the running_loss (0) with loss.item and assigns to running_loss\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.6563039422035217\n",
            "Training loss: 0.5708716315467183\n",
            "Training loss: 0.5304841820786639\n",
            "Training loss: 0.5009178184881443\n",
            "Training loss: 0.45129488117811156\n",
            "Training loss: 0.40192796617019466\n",
            "Training loss: 0.36930217357670386\n",
            "Training loss: 0.3514103649592981\n",
            "Training loss: 0.3278091113741805\n",
            "Training loss: 0.2758683842129824\n",
            "Training loss: 0.26220729401925713\n",
            "Training loss: 0.23746806601198708\n",
            "Training loss: 0.21095692284586953\n",
            "Training loss: 0.20572901480808492\n",
            "Training loss: 0.18635623974771034\n",
            "Training loss: 0.16978003211864612\n",
            "Training loss: 0.15551793965988042\n",
            "Training loss: 0.1615360117540127\n",
            "Training loss: 0.15097471580999652\n",
            "Training loss: 0.14795397676345778\n",
            "Training loss: 0.17469353619508626\n",
            "Training loss: 0.17176514627729975\n",
            "Training loss: 0.18872191666102991\n",
            "Training loss: 0.14682274098258194\n",
            "Training loss: 0.14062271335321228\n",
            "Training loss: 0.11641822373721658\n",
            "Training loss: 0.11053758886892621\n",
            "Training loss: 0.10833728917669959\n",
            "Training loss: 0.1001243541789491\n",
            "Training loss: 0.08981732165486347\n",
            "Training loss: 0.08324394397801016\n",
            "Training loss: 0.07499314980899416\n",
            "Training loss: 0.07713083362933702\n",
            "Training loss: 0.07328414083344907\n",
            "Training loss: 0.06783640266555112\n",
            "Training loss: 0.0655615300837359\n",
            "Training loss: 0.061919356991605064\n",
            "Training loss: 0.0705002411964304\n",
            "Training loss: 0.1100747558379137\n",
            "Training loss: 0.14462626025807568\n",
            "Training loss: 0.20702977869205358\n",
            "Training loss: 0.24917305760630748\n",
            "Training loss: 0.24255470086525127\n",
            "Training loss: 0.14162068795867083\n",
            "Training loss: 0.0960596008907731\n",
            "Training loss: 0.07811301277393884\n",
            "Training loss: 0.07002971097599806\n",
            "Training loss: 0.06476035016225423\n",
            "Training loss: 0.0635371207963766\n",
            "Training loss: 0.05964478005927692\n",
            "Training loss: 0.05823127232582831\n",
            "Training loss: 0.057130993412035265\n",
            "Training loss: 0.05647885860455018\n",
            "Training loss: 0.056117372504980646\n",
            "Training loss: 0.05567180190984977\n",
            "Training loss: 0.054283422284487\n",
            "Training loss: 0.053295978523858985\n",
            "Training loss: 0.05244798675781434\n",
            "Training loss: 0.05379070935163238\n",
            "Training loss: 0.0540063799417112\n",
            "Training loss: 0.052756102998434314\n",
            "Training loss: 0.05268370637829166\n",
            "Training loss: 0.05512579180645932\n",
            "Training loss: 0.05330619710304479\n",
            "Training loss: 0.05635414424170626\n",
            "Training loss: 0.05667765200615111\n",
            "Training loss: 0.0673221414511251\n",
            "Training loss: 0.07683764332223957\n",
            "Training loss: 0.1383417105847379\n",
            "Training loss: 0.3035154050988395\n",
            "Training loss: 0.32254543387126633\n",
            "Training loss: 0.20685093086667178\n",
            "Training loss: 0.17787082352471062\n",
            "Training loss: 0.10355394842421137\n",
            "Training loss: 0.08154966243792598\n",
            "Training loss: 0.06798142353754218\n",
            "Training loss: 0.06606214046875787\n",
            "Training loss: 0.06326508889451804\n",
            "Training loss: 0.060714023419167484\n",
            "Training loss: 0.05842646091563127\n",
            "Training loss: 0.05776186706072355\n",
            "Training loss: 0.056773031334869745\n",
            "Training loss: 0.05530516140620088\n",
            "Training loss: 0.05404774665196494\n",
            "Training loss: 0.052550480096947357\n",
            "Training loss: 0.05158573986107779\n",
            "Training loss: 0.052171221758926106\n",
            "Training loss: 0.04874539834293878\n",
            "Training loss: 0.047958112188342325\n",
            "Training loss: 0.04633416824480065\n",
            "Training loss: 0.04601762929459943\n",
            "Training loss: 0.045750416962219764\n",
            "Training loss: 0.0506835527964385\n",
            "Training loss: 0.04842034054397628\n",
            "Training loss: 0.04697185123151895\n",
            "Training loss: 0.04693844219990552\n",
            "Training loss: 0.0464780544716632\n",
            "Training loss: 0.045379553049115645\n",
            "Training loss: 0.045451122984892074\n",
            "Training loss: 0.07323106312709037\n",
            "Training loss: 0.06945285287390394\n",
            "Training loss: 0.09227161194629423\n",
            "Training loss: 0.20399639791831736\n",
            "Training loss: 0.18634116770018164\n",
            "Training loss: 0.1604484100258205\n",
            "Training loss: 0.11094497339572848\n",
            "Training loss: 0.0893560179577368\n",
            "Training loss: 0.08610400702895188\n",
            "Training loss: 0.07013888538974088\n",
            "Training loss: 0.06032617116579786\n",
            "Training loss: 0.05620728127045057\n",
            "Training loss: 0.05022797065150992\n",
            "Training loss: 0.05044123668950505\n",
            "Training loss: 0.05475096630852488\n",
            "Training loss: 0.05143183807944652\n",
            "Training loss: 0.049609707256710926\n",
            "Training loss: 0.05104731441909879\n",
            "Training loss: 0.05113423803449245\n",
            "Training loss: 0.051948115626412515\n",
            "Training loss: 0.05159681354629489\n",
            "Training loss: 0.05337120783543668\n",
            "Training loss: 0.05159815863518771\n",
            "Training loss: 0.04643258938822106\n",
            "Training loss: 0.0455317700877303\n",
            "Training loss: 0.04415530174695205\n",
            "Training loss: 0.046881818311947696\n",
            "Training loss: 0.048175761867286185\n",
            "Training loss: 0.0446940323298161\n",
            "Training loss: 0.051962219338215286\n",
            "Training loss: 0.0553532066107614\n",
            "Training loss: 0.06240751220924189\n",
            "Training loss: 0.09377624334699315\n",
            "Training loss: 0.12255766503967164\n",
            "Training loss: 0.15171650188361727\n",
            "Training loss: 0.13691680661470787\n",
            "Training loss: 0.1472017505864908\n",
            "Training loss: 0.11222852303124056\n",
            "Training loss: 0.06734484840161735\n",
            "Training loss: 0.05468142071665015\n",
            "Training loss: 0.043873593594492756\n",
            "Training loss: 0.04159014837009969\n",
            "Training loss: 0.04137460822534106\n",
            "Training loss: 0.043856448772158926\n",
            "Training loss: 0.04002367918203562\n",
            "Training loss: 0.03900681364633251\n",
            "Training loss: 0.03847214919813091\n",
            "Training loss: 0.03821705852879393\n",
            "Training loss: 0.03796356599834725\n",
            "Training loss: 0.03792198519250832\n",
            "Training loss: 0.03783437878971709\n",
            "Training loss: 0.037846405958478546\n",
            "Training loss: 0.03777652911656599\n",
            "Training loss: 0.03872424207071251\n",
            "Training loss: 0.037594268079662534\n",
            "Training loss: 0.03760335656749326\n",
            "Training loss: 0.037280553156473085\n",
            "Training loss: 0.03690597735680413\n",
            "Training loss: 0.0368802935753171\n",
            "Training loss: 0.036728330119974374\n",
            "Training loss: 0.03673042672274211\n",
            "Training loss: 0.036723441656582374\n",
            "Training loss: 0.03667594343074242\n",
            "Training loss: 0.03667397610408399\n",
            "Training loss: 0.036617653331010906\n",
            "Training loss: 0.036768198941522647\n",
            "Training loss: 0.03672235747341424\n",
            "Training loss: 0.03665531611087073\n",
            "Training loss: 0.03665143347883181\n",
            "Training loss: 0.03672879541320471\n",
            "Training loss: 0.03658985918437105\n",
            "Training loss: 0.03662409851783262\n",
            "Training loss: 0.036613438591125376\n",
            "Training loss: 0.036520800907388414\n",
            "Training loss: 0.03659098878628489\n",
            "Training loss: 0.03649976373424402\n",
            "Training loss: 0.036581864985325924\n",
            "Training loss: 0.03629146204614854\n",
            "Training loss: 0.03646922181662109\n",
            "Training loss: 0.03610962473985416\n",
            "Training loss: 0.03611529898827895\n",
            "Training loss: 0.036111582333765775\n",
            "Training loss: 0.03605157162835495\n",
            "Training loss: 0.03612682876019989\n",
            "Training loss: 0.036099444899463845\n",
            "Training loss: 0.03600969176992161\n",
            "Training loss: 0.03739687940052557\n",
            "Training loss: 0.06669613555987314\n",
            "Training loss: 0.141812230982795\n",
            "Training loss: 0.4164647654425807\n",
            "Training loss: 0.25842545962915187\n",
            "Training loss: 0.18295029841545152\n",
            "Training loss: 0.13832188779261054\n",
            "Training loss: 0.10852781347021824\n",
            "Training loss: 0.09187307293958417\n",
            "Training loss: 0.07334480725410508\n",
            "Training loss: 0.06498697608504898\n",
            "Training loss: 0.06493553773659032\n",
            "Training loss: 0.05997804078989003\n",
            "Training loss: 0.06231675952922825\n",
            "Training loss: 0.05468852142554924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "94clYCR0h6gH"
      },
      "source": [
        "**Evaluate Model Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4lKRk21Th6gI",
        "outputId": "a82e47e3-2420-4056-acc9-c2d7973b570b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "# Convert data into arrays\n",
        "xb = np.array(X_test_62, dtype = \"float32\")\n",
        "yb = np.array(y_test, dtype = \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "# Apply the model to the whole testing dataset\n",
        "ps = model(xb)\n",
        "\n",
        "#print('Probabilities', ps[:10])\n",
        "\n",
        "# Obtain the top probability\n",
        "top_p, top_class = ps.topk(1, dim=1)\n",
        "#print('true vals', yb[:10])\n",
        "\n",
        "# Drop the grad \n",
        "top_p = top_p.detach().numpy()\n",
        "top_class = top_class.detach().numpy()\n",
        "\n",
        "top_class = (top_p >= 0.5).astype(np.int)\n",
        "#print('top class', top_class[:10])\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(yb, top_p, pos_label=1)\n",
        "\n",
        "# Compute ROC area\n",
        "roc_auc = round(auc(fpr, tpr),3)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.3f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(yb, top_class))\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(yb, top_class))\n",
        "print('\\n')\n",
        "print(\"=== Accuracy Score ===\")\n",
        "print(\"Accuracy:\", round(accuracy_score(yb, top_class),3))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Confusion Matrix ===\n",
            "[[177 110]\n",
            " [ 70  80]]\n",
            "\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.62      0.66       287\n",
            "         1.0       0.42      0.53      0.47       150\n",
            "\n",
            "    accuracy                           0.59       437\n",
            "   macro avg       0.57      0.58      0.57       437\n",
            "weighted avg       0.62      0.59      0.60       437\n",
            "\n",
            "\n",
            "\n",
            "=== Accuracy Score ===\n",
            "Accuracy: 0.588\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwFtRH/CLwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ge7ywyF--6o5"
      },
      "source": [
        "# **Neural Network Experiments**\n",
        "\n",
        "**December 4th - 62 Principal Components, Neural Network Architecture:**\n",
        "\n",
        "**Loss:** Binary Cross Entropy Loss\n",
        "\n",
        "**Activation:** ReLU\n",
        "\n",
        "**Batchsize:** 32\n",
        "\n",
        "1) Hidden Layers (30), epochs = 100, Adam, LR = 0.003\n",
        "\n",
        "AUC: 0.604\n",
        "\n",
        "Accuracy: 0.611\n",
        "\n",
        "Recall: 0.45\n",
        "\n",
        "2) Hidden Layers (30), epochs = 100, Adam, LR = 0.001\n",
        "\n",
        "AUC: 0.574\n",
        "\n",
        "Accuracy: 0.609\n",
        "\n",
        "Recall: 0.41\n",
        "\n",
        "3) Hidden Layers (30), epochs = 200, Adam, LR = 0.003\n",
        "\n",
        "AUC: 0.580\n",
        "\n",
        "Accuracy: 0.6\n",
        "\n",
        "Recall: 0.41\n",
        "\n",
        "4) Hidden Layers (30, 10), epochs = 100, Adam, LR = 0.003\n",
        "\n",
        "AUC: 0.607\n",
        "\n",
        "Accuracy: 0.625\n",
        "\n",
        "Recall: 0.43\n",
        "\n",
        "5) Hidden Layers (30, 10), epochs = 200, Adam, LR = 0.003\n",
        "\n",
        "AUC: 0.613\n",
        "\n",
        "Accuracy: 0.613\n",
        "\n",
        "Recall: 0.41\n",
        "\n",
        "**6) Hidden Layers (30, 10), epochs = 200, Adam, LR = 0.001**\n",
        "\n",
        "**AUC: 0.6112**\n",
        "\n",
        "**Accuracy: 0.634**\n",
        "\n",
        "**Recall: 0.51 # higher Recall here**\n",
        "\n",
        "7) Hidden Layers (30, 10), epochs = 200, Adam, LR = 0.005\n",
        "\n",
        "AUC: 0.583 # poor performance\n",
        "\n",
        "Accuracy: 0.588\n",
        "\n",
        "Recall: 0.53 # higher Recall here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koIcDs3f92Xt",
        "colab_type": "text"
      },
      "source": [
        "# **Next Steps**\n",
        "\n",
        "1.   Compare performance of 1213, 62, and 33 prinicipal components with Neural Networks\n",
        "2.   Test Clinical Data usign the same training + testing set (maybe just save the data to a different file and then create a new script)\n",
        "3.   Assess performance of a clinical + pca model \n",
        "\n"
      ]
    }
  ]
}